# Hadoop的搭建

### 一.32和64位区别
---
有一种说法，hadoop官方提供的是在32位系统下编译的，linux 64位要在64系统下再编译一次。然而我发现，根本不是这个问题。浪费了我一上午。<br>
日日日。最后还是没有编译好（因为编译花太久时间了，抽空可以编译一下）。

下面给出参考资料：
更换hadoop本地库为64位版本：https://www.jianshu.com/p/22e64ea3b765<br>
安装hadoop的相关依赖：https://blog.csdn.net/scorpion_zs/article/details/53128949<br>
重新编译: https://blog.csdn.net/sl1992/article/details/54341407#23%E5%AE%89%E8%A3%85ant<br>
https://blog.csdn.net/lx940112/article/details/80384337<br>


### 二.hadoop部署
**hadoop目前问题：3.X和2.X版本部署不太一样，现在实验的是2.X版本**

#### 2.X版本部署
---
**1.java部署**<br>
下载java version "1.8.0_191"并进行配置(bash:~/.bashrc, zsh:~/.zshrc)
https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html
**2.hadoop部署**<br>
hadoop下载：
http://apache.mirrors.lucidnetworks.net/hadoop/common/hadoop-3.1.1/<br>
http://apache.claz.org/hadoop/common/

**3.配置**<br>
配置jdk路径和hadoop路径。
zsh编辑~/.zshrc，
bash编辑~/.bashrc。<br>
配置hadoop文件夹 etc/hadoop文件夹里的一些东西


#### 3.X版本配置
---
**1.java部署和hadoop部署**<br>
这个和2.X一样，都是要这样部署的

**2.~/.bashrc**<br>
```
export CLASSPATH=$:CLASSPATH:$JAVA_HOME/lib/  
export JAVA_HOME=/usr/local/jdk1.8.0_191  
export HADOOP_HOME=/usr/local/src/hadoop-3.1.1/  
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$JAVA_HOME/bin  
#export CLASSPATH=$:CLASSPATH:$JAVA_HOME/lib/  
#export HADOOP_HOME=/usr/local/src/hadoop-2.7.6  
#export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin :$MAVEN_HOME/bin:$ANT_HOME/bin:$PROTOC_HOME/bin:$MAVEN_HOME/bin  

export HADOOP_MAPRED_HOME=$HADOOP_HOME  
export HADOOP_COMMON_HOME=$HADOOP_HOME  
export HADOOP_HDFS_HOME=$HADOOP_HOME  
export YARN_HOME=$HADOOP_HOME  
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native  
export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib"  
#export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin  
export HADOOP_INSTALL=$HADOOP_HOME  
```

**3.编辑hadoop文件**<br>

>编辑etc/hadoop/hadoop-env.sh的JAVA_HOME

>编辑core-site.xml  
```
<configuration>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/Users/rick/workspace/hdfs/tmp</value>
        <description>A base for other temporary directories.</description>
    </property>
    <property>
        <name>fs.default.name</name>
        <value>hdfs://Rick:9000</value>
    </property>
</configuration>
```

>编辑hdfs-site.xml
```
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>2</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>/home/rick/workspace/hdfs/name</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>/home/rick/workspace/hdfs/data</value>
    </property>
</configuration>
```

> MapReduce和yarn功能需要编辑更多，但是目前只用到hdfs，所以只编辑了这么多。


### 三.Hadoop集群搭建
---
1. hdfs的dfs.replication代表有几个datanode
2. sudo vim /etc/hostname 修改主机名<br>
   sudo vim /etc/hosts 修改结点的ip映射
3. 配置SSH的无密码登录
```
cd ~/.ssh               # 如果没有该目录，先执行一次ssh localhost
rm ./id_rsa*            # 删除之前生成的公匙（如果有）
ssh-keygen -t rsa       # 一直按回车就可以
cat ./id_rsa.pub >> ./authorized_keys
```
4.  将类似的配置拷贝到datanode上。
5.  修改etc/hadoop/workers文件
```
#我的本机和另外一个机器为datanode
localhost
datanode1
```
6.  初始化并启动
```
hdfs namenode -format       # 首次运行需要执行初始化，之后不需要
cd hadoop-3.1.1 #进入hadoop文件夹
./sbin/start-all.sh #启动所有
./sbin/start-dfs.sh #启动hdfs
./sbin/stop-all.sh
./sbin/stop-dfs.sh

```
7. hadoop pyhon库hdfs
8. 可能会有权限问题sudo chmod -R 777

### 四. 参考资料
---
https://www.yiibai.com/hadoop/how_to_install_hadoop.html<br>
https://www.jianshu.com/p/3859f57aa545<br>
http://dblab.xmu.edu.cn/blog/install-hadoop-cluster/<br>
